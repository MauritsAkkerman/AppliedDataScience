{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Supervised Classification\n",
    "\n",
    "### Notebook created by [Bright Cape](https://brightcape.nl/)\n",
    "*Author: Maurits Akkerman*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents <a name=\"Contents\"></a>\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\n",
    "* [Step 1: Data Preprocessing](#Step-1:-Data-Preprocessing)\n",
    "\n",
    "    - [Under and Overfitting](#1.1:-Under-and-Overfitting)\n",
    "\n",
    "* [Step 2: Model Creation and Parameter Tuning](#Step-2:-Model-Creation-and-Parameter-Tuning)\n",
    "\n",
    "    - [Decision Tree](#2.1:-Decision-Tree)\n",
    "    \n",
    "        * [Classifier](#2.1.1:-Decision-Tree-Classifier)\n",
    "    \n",
    "        * [Regressor](#2.1.2:-Decision-Tree-Regressor)\n",
    "\n",
    "    - [Random Forests](#2.2:-Random-Forests)\n",
    "\n",
    "    - [K-Nearest Neighbor](#2.3:-K-Nearest-Neighbor)\n",
    "\n",
    "* [Step 3: Model Validation](#Step-3:-Model-Validation)\n",
    "* [Conclusion](#Conclusion) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"Introduction\"></a>\n",
    "\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "In the previous notebooks ([Data Cleaning](https://colab.research.google.com/github/MauritsAkkerman/AppliedDataScience/blob/main/Data%20Cleaning%20%26%20Visualization.ipynb), [Linear Regression](https://colab.research.google.com/github/MauritsAkkerman/AppliedDataScience/blob/main/Linear%20Regression.ipynb)), we showed the time and effort required to properly clean datasets. Additionally, we showed how to set up a linear regression model. This notebook will focus on the subsequent modelling steps and show how to set up a supervised machine learning model.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1yX1hvNeepHnm1O58voKm_qQultGcpNOy\" style=\"width: 800px;\" />\n",
    "\n",
    "\n",
    "Machine Learning Algorithms (MLA) can be categorized into two classes: supervised and unsupervised learning. In supervised learning, we know what the outcome of our model should be. We provide our model with examples and provide the results of these examples. Then, we provide our model with new examples and ask the model to predict the outcomes. In unsupervised learning, we do not know what the outcome of our model should be. These models are used for discovering similarities, structures, and associations.\n",
    "\n",
    "In our case, we want to learn more about the transaction behavior of google analytics customers. Therefore, the outcome of the model is known. Namely, we want to predict whether a transaction takes place, and if so what the size of that transaction would be.\n",
    "\n",
    "This notebook is set up in the following manner: First, we will include some steps required to use our cleaned data in MLA, we will discuss two commonly used method Decision Trees with the extension to Random Forests, and K-Nearest Neighbour. Finally, the performance of each model is compared and the best is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "orig_url='https://drive.google.com/file/d/1SJWwLTX6lHKP13A-PudfRnuN7s-_5vxy/view?usp=sharing'\n",
    "dwn_url='https://drive.google.com/uc?export=download&id=' + orig_url.split('/')[-2]\n",
    "url = requests.get(dwn_url).text\n",
    "csv_raw = StringIO(url)\n",
    "\n",
    "# Read the data into a pandas data frame and show the first 5 rows\n",
    "data = pd.read_csv(csv_raw, index_col=0, sep=',')\n",
    "data['date'] = pd.to_datetime(data['date'],  format='%Y-%m-%d', errors='coerce')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing <a name=\"Step-1:-Data-Preprocessing\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "Simply dumping all available data in a machine learning model is often not possible. First, several data transformation steps should be performed. For example, most machine learning algorithms can't deal with categorical variables such as \"medium\". To retain the information we need to transform it into multiple features. For each possible category of the original variable, a new binary feature is defined. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data['medium'].copy()\n",
    "subset = pd.get_dummies(data = subset)\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides these transformations, to achieve our goal of whether a transaction occurred yes or no, we add a variable called \"transaction\" that provides us with just this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset\n",
    "subset = data.copy()\n",
    "\n",
    "# Add a binary variable called transaction that indicates wheter a transaction occured.\n",
    "# For example a totalTransaction value of 100 indicates a transaction took place and hence the variable transaction is set to 1.\n",
    "subset['transaction'] = np.where(subset['totalTransactionRevenue'] > 0, 1, 0).tolist()\n",
    "\n",
    "# Add a month and year variable to prevent overfitting on dates. More on over and underfitting in the next subsection.\n",
    "subset['month'] = subset['date'].dt.month\n",
    "subset['year'] = subset['date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an intuitive understanding of the factors influencing whether a transaction occured, we make some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variables and their effect on whether a transaction occured\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.catplot(x=\"campaign\", y=\"transaction\", data=subset,kind=\"bar\")\n",
    "ax.set_ylabels(\"transaction\")\n",
    "ax.set_xticklabels(ha='right', rotation=45)\n",
    "\n",
    "ax = sns.catplot(x=\"medium\", y=\"transaction\", data=subset,kind=\"bar\")\n",
    "ax.set_ylabels(\"transaction\")\n",
    "ax.set_xticklabels(ha='right', rotation=45)\n",
    "\n",
    "ax = sns.catplot(x=\"isMobile\", y=\"transaction\", data=subset,kind=\"bar\")\n",
    "ax.set_ylabels(\"transaction\")\n",
    "ax.set_xticklabels(ha='right', rotation=45)\n",
    "\n",
    "ax = sns.catplot(x=\"continent\", y=\"transaction\", data=subset,kind=\"bar\")\n",
    "ax.set_ylabels(\"transaction\")\n",
    "ax.set_xticklabels(ha='right', rotation=45)\n",
    "\n",
    "ax = sns.catplot(x=\"month\", y=\"transaction\", data=subset,kind=\"bar\")\n",
    "ax.set_ylabels(\"transaction\")\n",
    "ax.set_xticklabels(ha='right', rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we can observe that most variables have a clear effect on a transaction occurring. However, when taking a closer look at the month variable, only in March not all observations were transactions. If we dive deeper into this observation, we find that march has over 50% of the total records. Thus it appears that in the data collection only the non-transactions in March were collected and not for any other months.\n",
    "\n",
    "This observation requires us, data scientist, to question the data provider. Is there an explanation for this observation? Is data missing?\n",
    "\n",
    "It turns out that the provided dataset is balanced, having equal transactions and non-transactions. The consequence of only having non-transaction observations in March is that we can not use the date variable as a prediction, as it would be unrealistic to assume no non-transaction occurred during the other months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the date variables as clearly there the non-transaction were only recorded during the month March\n",
    "subset.drop(['date', 'month', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# Since using a single data likely overfits the model\n",
    "subset = pd.get_dummies(data = subset)\n",
    "\n",
    "# Show an example of the top rows of the dataset\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Under and Overfitting <a name=\"1.1:-Under-and-Overfitting\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "Providing a MLA with all available data is a common mistake in data science projects. After all, more data is better, right? Unfortunately, no. A model can be trained to 100% accurately predict the outcome based on a dataset. However, when we then use new observations slightly different from the original dataset, wrong predictions can occur. The figure below provides a good example of under and overfitting:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gVlMa3l2Rm8ulUoCVErA_ov8zpyzKsnK\" style=\"width: 600px;\" />\n",
    "\n",
    "Now, what do we do to prevent overfitting? First, we split the total data into two: a training data set and a testing data set. The percentage of the data that should be used for the training and testing set is usually set at 75% training and 25% testing. This split can be altered to include more or less training data. In the end, there is always a tradeoff between using as much training data to create the best set up of the model, while still having enough data to get a representative testing sample to unbiasedly judge it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using all our variables EXCEPT for the totalTransactionRevenue and transaction to predict our variables\n",
    "\n",
    "# Note that scikit-learn expects each entry to be a list of values, e.g.,\n",
    "# [ [val1, val2, val3],\n",
    "#   [val1, val2, val3],\n",
    "#   ... ]\n",
    "# such that our input data set is represented as a list of lists\n",
    "\n",
    "# We can extract the data in this format from pandas like this:\n",
    "all_inputs = subset.drop(columns=['totalTransactionRevenue', 'transaction']).values\n",
    "\n",
    "# Similarly, we can extract the class labels\n",
    "all_labels = subset['transaction'].values\n",
    "\n",
    "# Import the function to divide the dataset in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more robust version of splitting the data into test and training sets is to use cross-validation. This methodology iteratively trains and tests the model on different subsets of the data. The actual model performance is then determined based on the average performance over all these cross-validations. An example of cross-validation with size 5 is provided in the image below: \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1LVI3FpUSCYPFYNohGPDZQa9Binh3Mjgm\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic of cross-validation is revisited in the next chapter. Here, we discuss and show the differences in accuracy between the \"normal\" approach and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Creation and Parameter Tuning<a name=\"Step-2:-Model-Creation-and-Parameter-Tuning\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "## 2.1: Decision Tree <a name=\"2.1:-Decision-Tree\"></a>\n",
    "We'll start with a model called the Decision Tree. Some fancier models give more accurate predictions, but decision trees are easy to understand. Further, they are at the foundation of some of the best models in data science.\n",
    " \n",
    "A decision tree consists of nodes and leaves connected by branches. A node is a decision point. For example, we decide which branch to follow based on whether the number of hits is larger than 40. A leaf is an endpoint of the tree and makes the prediction. The prediction of one observation is therefore determined by its variables and is set by the leaf it ends on.\n",
    "\n",
    "In this notebook, we consider two types of trees, classifiers and regressors. A classifier classifies an observation as either \"yes\" or \"no\". For the google analytics data this would mean: \"did a sale take place?\" to which the decision tree classifier predicts either \"yes\" or \"no\". A regressor estimates the value of an observation. For the same example, the output question would be: \"how much was the total transaction revenue\" to which the decision tree regressor will predict for example 450 (or any other number, including 0).\n",
    "\n",
    "Now let's take a look at both a classifier example as well as a regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1: Decision Tree Classifier <a name=\"2.1.1:-Decision-Tree-Classifier\"></a>\n",
    "\n",
    "We have already defined the classifier labels above and thus we can instantly fit our model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "\n",
    "# Check how accurate the classifier predicts whether a transaction took place\n",
    "print('Accuracy of the Decision Tree Classifier {:.2%}'.format(decision_tree_classifier.score(testing_inputs, testing_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with 93.75% accuracy, we predicted whether a transaction took place or not based on the input parameters. We can further investigate how this tree is created and of which nodes and leaves it exists. \n",
    "\n",
    "The depth of a tree is the number of decision points between the input and output of the decision tree. The depth of this tree has not been specified. Since it would be extremely confusing to show the entire tree, we stop further branching when reaching a depth of three nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "text_representation = export_text(decision_tree_classifier, feature_names = subset.columns.drop(['transaction', 'totalTransactionRevenue']).tolist(), max_depth = 3, spacing = 8)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out how we should read this tree. Consider an observation that has 8 pageviews. The first node is: \"pageviews <= 8.5\", and consequently a little further down: \"pageviews > 8.5\". As our example had 8 pageviews we follow the first branch and move to the second node. The question now is \"pageviews <= 4.5\". In our case it is not and therefore, we follow the second branch. And so on, until a leaf is reached. We observe several leaves in the first branch. These leaves state \"class: 0\" or \"class: 1\" and thus, provides the outcome of the tree.\n",
    "\n",
    "The created decision tree is based on the training dataset and its accuracy is determined by the testing dataset. What would happen if we change these training and testing sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(500):\n",
    "    # Make new testing and training datasets\n",
    "    (training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25)\n",
    "    \n",
    "    # Run the decision tree classifier again and determine it's score\n",
    "    decision_tree_classifier = DecisionTreeClassifier(random_state=1)\n",
    "    decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing_classes)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "    \n",
    "plt.hist(model_accuracies, bins = 10)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results of the decision tree are all over the place. They are extremely dependent on which training- and testingset you provide. This is an example of overfitting. As explained earlier we can balance the amount of overfitting by using cross-validation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_score returns a list of the scores, taking the average provides a good estimate of the model performance.\n",
    "# This method is a lot less sensitive to what training or testing sets are provided to the model and therefore provides\n",
    "# a better measure of the true model performance.\n",
    "cv_scores = cross_val_score(DecisionTreeClassifier(random_state=1), all_inputs, all_labels, cv=10)\n",
    "print('Average accuracy of the Decision Tree classifier: {:.2%}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an average accuracy of 93.53% based on 10 different splits of the training and testing data. This score is much more robust than the previous method. If we perform the same iteration of 100 different scores, we can see that the range of the decision tree scores is much tighter relative to the image above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(100):\n",
    "    # Run the decision tree classifier again but now with cross validation and determine the average score\n",
    "    cv_scores = cross_val_score(DecisionTreeClassifier(), all_inputs, all_labels, cv=10)\n",
    "    model_accuracies.append(np.mean(cv_scores))\n",
    "\n",
    "plt.hist(model_accuracies, bins = 10)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning <a name=\"Parameter-tuning\"></a>\n",
    "\n",
    "Before, we mentioned the parameter depth of a tree. This parameters can be optimized to increase the performance of the model. Another parameter is the maximum number of variables to consider. There are many more variables to change and tweak, but for now, let's focus on these two. Subsequently, we can look at each combination of depth and variables and visualize the performance of the tree in a grid. This approach is called a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Specify the parameters we want to tune, in this case the depth and number of variables.\n",
    "number_depth = 10\n",
    "number_features = 9\n",
    "parameter_grid = {'max_depth': [i+1 for i in range(number_depth)],\n",
    "                  'max_features': [i+1 for i in range(number_features)]}\n",
    "\n",
    "# Initalize the grid search\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=1),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "# Fit the grid search based on the inputs and classes.\n",
    "grid_search.fit(all_inputs, all_labels)\n",
    "\n",
    "# Visualize the performance of sets of input parameters\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "grid_visualization = grid_search.cv_results_['mean_test_score']\n",
    "grid_visualization.shape = (number_depth, number_features)\n",
    "sns.heatmap(grid_visualization, cmap='Blues', annot=True)\n",
    "ax.set_xticklabels(grid_search.param_grid['max_features'], ha = 'center')\n",
    "ax.set_yticklabels(grid_search.param_grid['max_depth'], ha = 'center')\n",
    "ax.set_xlabel('max_features')\n",
    "ax.set_ylabel('max_depth')\n",
    "\n",
    "# Return the best classifier\n",
    "decision_tree_classifier = grid_search.best_estimator_\n",
    "print('Accuracy of the best Decision Tree Classifier: {:.2%}'.format(grid_search.best_score_),\n",
    "     'Best Performance found with a max depth: {} and max features: {}'.format(decision_tree_classifier.max_depth, decision_tree_classifier.max_features),\n",
    "     sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best Classification Tree we can create is a tree with a max depth of 5 nodes and a max number of features of 9. By using these features, we increase our performance from 93.75% to over 95.36%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2: Decision Tree Regressor <a name=\"2.1.2:-Decision-Tree-Regressor\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "Compared to the Decision Tree Classifier we now have to change our output. The output for the classifier was a binary variable indicating whether a transaction occurred, whereas now we are trying to predict what the expected revenue of a transaction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs stay the same, but we change the label from transaction to totalTransactionRevenue\n",
    "all_labels = subset['totalTransactionRevenue'].values\n",
    "\n",
    "# Import the Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the regressor\n",
    "decision_tree_regressor = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Check the score of the created model through the k-fold cross validation method\n",
    "cv_scores = cross_val_score(decision_tree_regressor, all_inputs, all_labels, cv=10)\n",
    "print('Average accuracy score: {:.2%}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW what happened there, suddenly the prediction is way off. Keep in mind that a score of 0.0 normally indicates that we make a prediction without looking at the inputs. Let's look at what type of predictions we would make with this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)\n",
    "decision_tree_regressor.fit(training_inputs, training_classes)\n",
    "prediction = decision_tree_regressor.predict(testing_inputs)\n",
    "\n",
    "for i in range(15):\n",
    "    print('{}: Predicted: {:.1f} Actual: {:.1f}'.format(i,prediction[i],testing_classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this problem is caused by the input. When predicting the exact value of the total transaction revenue the effect of extreme values could be an issue. To explain why let's consider the following example. A leaf has 40 samples with a transaction revenue between 0 and 50, let's say 25 on average, and 1 observation with a revenue of 1000. The predicted revenue for that leaf is determined by the average of all observations. In this case, the average would be 48 $(=\\frac{25 \\cdot 40 + 1 \\cdot 1000}{41})$. The estimation of 48 now overestimates the revenue for the majority of observations and extremely underestimates it for the extreme value. \n",
    "\n",
    "Looking at the predictions made above, we can see that for observation the predicted value is 111.9, while the actual value is 21. This could potentially be caused by just such an example.\n",
    "\n",
    "Maybe we can increase the score of our model by looking at the majority of the observations. So let's remove the 10% highest revenue observations.\n",
    "\n",
    "Additionally, let's only look at the rows where a transaction occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where no transaction occured\n",
    "subset2 = subset[subset['transaction'] != 0]\n",
    "\n",
    "# Respecify the inputs and labels for the MLA\n",
    "all_inputs2 = subset2.drop(columns=['transaction', 'totalTransactionRevenue']).values\n",
    "all_labels2 = subset2['totalTransactionRevenue'].values\n",
    "\n",
    "# Create the regressor\n",
    "decision_tree_regressor = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Check the score of the created model through the k-fold cross validation method\n",
    "cv_scores = cross_val_score(decision_tree_regressor, all_inputs2, all_labels2, cv=10)\n",
    "print('Average accuracy score: {:.2%}'.format(np.mean(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly that didn't solve our problem. One last option would be to look at the parameters, but due to the low score, it is highly unlikely that the revenue can be predicted based on our current input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parameters we want to tune, in this case the depth and number of variables.\n",
    "number_depth = 5\n",
    "number_features = 5\n",
    "parameter_grid = {'max_depth': [i+1 for i in range(number_depth)],\n",
    "                  'max_features': [i+1 for i in range(number_features)]}\n",
    "\n",
    "# Initalize the grid search\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=1),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "# Fit the grid search based on the inputs and classes.\n",
    "grid_search.fit(all_inputs2, all_labels2)\n",
    "\n",
    "# Visualize the performance of sets of input parameters\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "grid_visualization = grid_search.cv_results_['mean_test_score']\n",
    "grid_visualization.shape = (number_depth, number_features)\n",
    "sns.heatmap(grid_visualization, cmap='Blues', annot=True)\n",
    "ax.set_xticklabels(grid_search.param_grid['max_features'], ha='center')\n",
    "ax.set_yticklabels(grid_search.param_grid['max_depth'], ha='center')\n",
    "ax.set_xlabel('max_features')\n",
    "ax.set_ylabel('max_depth')\n",
    "\n",
    "# Return the best classifier\n",
    "decision_tree_regressor = grid_search.best_estimator_\n",
    "print('Accuracy of the best Decision Tree Regressor: {:.2%}'.format(grid_search.best_score_),\n",
    "     'Best Performance found with a max depth: {} and max features: {}'.format(decision_tree_regressor.max_depth, decision_tree_regressor.max_features),\n",
    "     sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, the score does not get much better. From these analyses, we can conclude that current input parameters do not provide enough information to predict the height of the revenue. The remainder of this notebook, therefore, focusses on classifying a transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Random Forests <a name=\"2.2:-Random-Forests\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "In the previous models, we looked at a single decision tree. But what if we combined the power of multiple trees to make our prediction? That is exactly what a random forest is. It trains multiple different trees and bases its output on the average outcome of those trees.\n",
    "\n",
    "When using classifiers, the class that is predicted is based on the majority class of the individual tree predictions. While for Regressors a simple average of all predictions is used.\n",
    "\n",
    "Since we concluded that determining the exact value of a transaction is not viable. We only focus on predicting whether a transaction is going to occur.\n",
    "\n",
    "The following blocks of code perform the same steps as before, looking at the initial accuracy of the random forest and applying a grid search to optimize the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output variable back to transaction\n",
    "all_labels = subset['transaction'].values\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators = 10, random_state = 1)\n",
    "\n",
    "# Check the score of the created model through the k-fold cross validation method\n",
    "cv_scores = cross_val_score(random_forest_classifier, all_inputs, all_labels, cv=10)\n",
    "print('Average accuracy of the Random Forest Classifier: {:.2%}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "parameter_grid = {'n_estimators': [10, 25, 50, 100],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [2, 4, 6, 8]}\n",
    "\n",
    "# Initalize the grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state = 1),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "# Fit the grid search based on the inputs and classes.\n",
    "grid_search.fit(all_inputs, all_labels)\n",
    "\n",
    "# Return the best classifier\n",
    "random_forest_classifier = grid_search.best_estimator_\n",
    "print('Accuracy of the best Random Forest model: {:.2%}'.format(grid_search.best_score_),\n",
    "     'Best Performance found with {} estimators and {} features'.format(random_forest_classifier.n_estimators, random_forest_classifier.max_features),\n",
    "     sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to a single tree with 95.36% accuracy we have now improved to 95.79% accuracy. While this example is not an extreme increase. Generally speaking, Random forests are much more powerful compared to Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: K-Nearest Neighbor <a name=\"2.3:-K-Nearest-Neighbor\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "The K-nearest neighbor algorithm is less intuitive than the decision tree. It predicts the value of an observation based on it's closest K neighbors. An example is given in the picture below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=17t6BFgqurWas3QNihxNPHt7u7qGFkHdY\" style=\"width: 300px;\"/>\n",
    "\n",
    "Let's perform the same steps as for the previous models and optimize our model parameters through a grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set K neighbor equal to 3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Check the score of the created model through the k-fold cross validation method\n",
    "cv_scores = cross_val_score(knn, all_inputs, all_labels, cv=10)\n",
    "print('Average accuracy of a KNN model with {} neighbors: {:.2%}'.format(knn.n_neighbors, np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "parameter_grid = {'n_neighbors': [i for i in range(1,25)]}\n",
    "\n",
    "# Initalize the grid search\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10)\n",
    "\n",
    "# Fit the grid search based on the inputs and classes.\n",
    "grid_search.fit(all_inputs, all_labels)\n",
    "\n",
    "# Visualize performance of number of neighbors\n",
    "plt.plot(range(1,25),grid_search.cv_results_['mean_test_score'])\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "# Return the best classifier\n",
    "knn = grid_search.best_estimator_\n",
    "print('Accuracy of the best KNN model: {:.2%}'.format(grid_search.best_score_),\n",
    "     'Best Performance found with {} neighbors'.format(knn.n_neighbors),\n",
    "     sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model uses 3 neighbors to make a prediction. Interestingly, this results in a worse accuracy compared to the decision trees and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Validation <a name=\"Step-3:-Model-Validation\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "In Machine Learning, there is no such thing as one \"best\" model. Rather we try to apply multiple different models and see which ones outperform others. In this case, let's compare the three models we have created so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df = pd.DataFrame({'accuracy': cross_val_score(decision_tree_classifier, all_inputs, all_labels, cv=10),\n",
    "                      'classifier': ['Decision Tree'] * 10})\n",
    "rf_df = pd.DataFrame({'accuracy': cross_val_score(random_forest_classifier, all_inputs, all_labels, cv=10),\n",
    "                       'classifier': ['Random Forest'] * 10})\n",
    "knn_df = pd.DataFrame({'accuracy': cross_val_score(knn, all_inputs, all_labels, cv=10),\n",
    "                      'classifier': ['K-Nearest Neighbours'] * 10})\n",
    "two_df = dt_df.append(rf_df)\n",
    "all_df = two_df.append(knn_df)\n",
    "\n",
    "sns.boxplot(x='classifier', y='accuracy', data=all_df)\n",
    "sns.stripplot(x='classifier', y='accuracy', data=all_df, jitter=True, color='black')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed earlier, the Decision Tree and Random Forest Classifier perform almost equal and the K-nearest neighbor algorithm performs slightly worse.\n",
    "\n",
    "Besides looking at just one indicator of accuracy, we can further divide our predictions into four categories: true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Depending on the business question at hand, some categories might carry more weight than others. For example, if we are trying to predict product failure of a vital component in an airplane, a false negative is extremely dangerous, whereas a false positive is less important.\n",
    "\n",
    "To distinguish these criteria, three metrics are defined.\n",
    "- Precision: Answers the question of how many selected items are relevant? How many of the classes that we predicted as positive are actually positive?\n",
    "- Recall: Answers the question of how many relevant items are selected? How many of the total positive cases are identified as positive\n",
    "- F1: A measure that combines the metrics precision and recall, providing a decision based on a single metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25)\n",
    "    \n",
    "predictions = decision_tree_classifier.predict(testing_inputs)\n",
    "\n",
    "matrix = confusion_matrix(testing_classes, predictions)\n",
    "print(\"Decision Tree Results:\",\n",
    "    \"True Positives: {}\".format(matrix[0][0]),\n",
    "     \"False Positives: {}\".format(matrix[1][0]), \n",
    "     \"True Negatives: {}\".format(matrix[0][1]),\n",
    "     \"False Negatives: {}\".format(matrix[1][1]),\n",
    "      classification_report(testing_classes, predictions),\n",
    "      sep='\\n')\n",
    "\n",
    "print()\n",
    "\n",
    "predictions = random_forest_classifier.predict(testing_inputs)\n",
    "matrix = confusion_matrix(testing_classes, predictions)\n",
    "print(\"Random Forest Results:\",\n",
    "    \"True Positives: {}\".format(matrix[0][0]),\n",
    "     \"False Positives: {}\".format(matrix[1][0]), \n",
    "     \"True Negatives: {}\".format(matrix[0][1]),\n",
    "     \"False Negatives: {}\".format(matrix[1][1]),\n",
    "      classification_report(testing_classes, predictions),\n",
    "      sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the sample we used, we perfectly predicted the classes by using the random forest classifier!\n",
    "\n",
    "Though in reality, when we want to compare different MLA, we would need another testing set. This set should be seperates at the first stage of the analysis as during the optimization of parameters we actually used the current testing set. So in essence we are again overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "[[ go back to the top ]](#Contents)\n",
    "\n",
    "This notebook serves as an example of how machine learning is used in practice. It also shows that not all machine learning algorithms necessarily deliver a valuable outcome in business contexts. We always have to keep in mind, what do we want to achieve with our model? and sometimes the answer is simply: we do not have enough or the right data.\n",
    "\n",
    "Remember one of the main mottos of data science: __\"Better Data Beast Fancier Algorithms\"__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
